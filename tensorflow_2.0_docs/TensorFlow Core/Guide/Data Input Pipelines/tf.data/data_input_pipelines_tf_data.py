# -*- coding: utf-8 -*-
"""Data Input Pipelines - tf.data.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1G_r0lexD-7Ayx4rMPd08q44dfEKsxyZJ
"""

from IPython.display import display, clear_output, HTML 
HTML('<img align="center" src="https://www.tensorflow.org/images/tf_logo_social.png" width="100%">')

"""### Importing the libraries and dependencies"""

# Commented out IPython magic to ensure Python compatibility.
from __future__ import print_function, absolute_import, division

import os
os.environ["PYTHONHASHSEED"] = str(101)
import gc
gc.enable()

import tensorflow as tf
import pathlib 
import numpy as np 
import pandas as pd 
import matplotlib.pyplot as plt 
# %matplotlib inline
np.set_printoptions(precision=4)
try:
#   %tensorflow_version 2.x
except:
  pass

print(tf.__version__)

!pip install --upgrade tensorflow

"""### Basic Mechanics"""

dataset = tf.data.Dataset.from_tensor_slices([8,3,0,8,2,1])
dataset

for el in dataset:
  print(el.numpy())

it = iter(dataset)
print(next(it).numpy())

print(dataset.reduce(0, lambda state, value: state + value).numpy())

"""### Dataset Structure"""

dataset1 = tf.data.Dataset.from_tensor_slices(tf.random.uniform([4,10]))
dataset1.element_spec

dataset2 = tf.data.Dataset.from_tensor_slices((tf.random.uniform([4,10]),
                                              tf.random.uniform([4,100], maxval=100, dtype=tf.int32)))
dataset2.element_spec

dataset3 = tf.data.Dataset.zip((dataset1, dataset2))
dataset3.element_spec

dataset4 = tf.data.Dataset.from_tensors(tf.SparseTensor([[0,0],[1,2]],values=[1,2],dense_shape=[3,4]))
dataset4.element_spec.value_type

dataset = tf.data.Dataset.from_tensor_slices(
    tf.random.uniform([4,10],minval=1,maxval=100,dtype=tf.float32)
)
print(dataset.element_spec)
for z in dataset:
  print(z.numpy())

dataset2 = tf.data.Dataset.from_tensor_slices(
    (tf.random.uniform([4]),
    tf.random.uniform([4,100], maxval=100, dtype=tf.int32)
))
dataset2

dataset2.element_spec

dataset3 = tf.data.Dataset.zip((dataset1,dataset2))
dataset3.element_spec

dataset3

for i in dataset3:
  print(i)

for a, (b,c) in dataset3:
  print(f"{a.shape}, {b.shape}, {c.shape}")

"""### Reading Input Data

#### Consuming NumPy arrays
"""

train, test = tf.keras.datasets.fashion_mnist.load_data()

images, labels = train
images = images / 255.0 
train_dataset = tf.data.Dataset.from_tensor_slices((images, labels))

BATCH_SIZE = 64
SHUFFLE_BUFFER_SIZE = 100

train_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)

model = tf.keras.Sequential([
    tf.keras.layers.Flatten(input_shape=(28, 28)),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(10, activation='softmax')
])

model.compile(optimizer=tf.keras.optimizers.RMSprop(),
                loss=tf.keras.losses.SparseCategoricalCrossentropy(),
                metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])

model.fit(train_dataset, epochs=5)

HTML('<p style="color:#95B9C7;"><strong>Note:</strong>  The above code snippet will embed the features and labels arrays in your TensorFlow graph as tf.constant() operations. This works well for a small dataset, but wastes memory---because the contents of the array will be copied multiple times---and can run into the 2GB limit for the tf.GraphDef protocol buffer.</p>')

"""#### Consuming Python Generators"""

from IPython.display import display, clear_output, HTML
HTML('<p style="color:orangered;"><strong>Caution:</strong> While this is a convienient approach it has limited portability and scalibility. It must run in the same python process that created the generator, and is still subject to the <a href="https://en.wikipedia.org/wiki/Global_interpreter_lock">Python GIL.</a></p>')

"""*  The Dataset.from_generator constructor converts the python generator to a fully functional tf.data.Dataset.

  *  The constructor takes a callable as input, not an iterator. This allows it to restart the generator when it reaches the end. It takes an optional args argument, which is passed as the callable's arguments.

  *  The output_types argument is required because tf.data builds a tf.Graph internally, and graph edges require a tf.dtype.
"""

def count(stop):
  i = 0
  while i < stop:
    yield i
    i += 1

for i in count(10):
  print(i)

ds_counter = tf.data.Dataset.from_generator(count, args=[50], output_types=tf.int32, output_shapes=())

ds_counter.element_spec

for count_batch in ds_counter.repeat().batch(10).take(5):
  print(count_batch.numpy())

for count_batch in ds_counter.repeat().batch(5).take(5):
  print(count_batch.numpy())

for count_batch in ds_counter.repeat().batch(5).take(20):
  print(count_batch.numpy())

def gen_series():
  i = 0
  while True:
    size = np.random.randint(0, 10)
    yield i, np.random.normal(size=(size,))
    i += 1

for i, series in gen_series():
  print(i, ":", str(series))
  if i>5:
    break

ds_series = tf.data.Dataset.from_generator(
    gen_series,
    output_types = (tf.int32, tf.float32),
    output_shapes = ((), (None,))
)

ds_series

ds_series_batch = ds_series.shuffle(20).padded_batch(10, padded_shapes=([], [None]))

ids, sequence_batch = next(iter(ds_series_batch))
print(ids.numpy())
print()
print(sequence_batch.numpy())

"""##### Wrapping preprocessing.image.ImageDataGenerator as tf.data.Dataset"""

flowers = tf.keras.utils.get_file(
    'flower_photos',
    'https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz',
    untar = True
)

flowers

img_gen = tf.keras.preprocessing.image.ImageDataGenerator(
    rescale=1/255.,
    rotation_range = 20
)

images, labels = next(img_gen.flow_from_directory(flowers))

print(images.dtype, images.shape)
print(labels.dtype, labels.shape)

ds = tf.data.Dataset.from_generator(
    img_gen.flow_from_directory, 
    output_types=(tf.float32, tf.float32),
    output_shapes=([32,256,256,3],[32,5])
)

print(ds)

# for value in ds.take(2):
#   print(value)

"""#### Consuming TFRecord Data

The tf.data API supports a variety of file formats so that you can process large datasets that do not fit in memory. For example, the TFRecord file format is a simple record-oriented binary format that many TensorFlow applications use for training data. The tf.data.TFRecordDataset class enables you to stream over the contents of one or more TFRecord files as part of an input pipeline.
"""

fsns_test_file = tf.keras.utils.get_file('fsns.tfrec', "https://storage.googleapis.com/download.tensorflow.org/data/fsns-20160927/testdata/fsns-00000-of-00001")

"""The filenames argument to the TFRecordDataset initializer can either be a string, a list of strings, or a tf.Tensor of strings. Therefore if you have two sets of files for training and validation purposes, you can create a factory method that produces the dataset, taking filenames as an input argument:"""

dataset = tf.data.TFRecordDataset(filenames=[fsns_test_file])

dataset.element_spec

dataset

"""Many TensorFlow projects use serialized tf.train.Example records in their TFRecord files. These need to be decoded before they can be inspected:"""

raw_example = next(iter(dataset))
print(raw_example)
print(raw_example.numpy())
print(raw_example.dtype)

parsed = tf.train.Example.FromString(raw_example.numpy())

parsed

parsed.features.feature['image/text']

parsed.features.feature["image/width"]

parsed.features.feature["image/class"]

"""#### Consuming text data

Many datasets are distributed as one or more text files. The tf.data.TextLineDataset provides an easy way to extract lines from one or more text files. Given one or more filenames, a TextLineDataset will produce one string-valued element per line of those files.
"""

directory_url = 'https://storage.googleapis.com/download.tensorflow.org/data/illiad/'
file_names = ['cowper.txt', 'derby.txt', 'butler.txt']

file_path = [
             tf.keras.utils.get_file(file_name, directory_url+file_name) 
             for file_name in file_names
]

file_path

dataset = tf.data.TextLineDataset(file_path)
print(dataset.element_spec)
print()
for line in dataset.take(10):
  print(line.numpy())

files_ds = tf.data.Dataset.from_tensor_slices(file_path)
print(files_ds)

lines_ds = files_ds.interleave(tf.data.TextLineDataset, cycle_length=3)
for i, line in enumerate(lines_ds.take(9)):
  if i%3==0:
    print()
  print(line.numpy())

titanic_file = tf.keras.utils.get_file('train.csv', "https://storage.googleapis.com/tf-datasets/titanic/train.csv")
titanic_lines = tf.data.TextLineDataset(titanic_file)

for line in titanic_lines.take(10):
  print(line.numpy())

def survived(line):
  return tf.not_equal(tf.strings.substr(line,0,1), "0")

survivors = titanic_lines.skip(1).filter(survived)

x = 0
for i in survivors:
  print(i.numpy())
  x += 1
  if x>5:
    break

"""#### Consuming CSV files"""

titanic_file = tf.keras.utils.get_file("train.csv", "https://storage.googleapis.com/tf-datasets/titanic/train.csv")

df = pd.DataFrame.from_csv(titanic_file, index_col=None)
df.head()

titanic_slices = tf.data.Dataset.from_tensor_slices(dict(df))
for feature_batch in titanic_slices.take(1):
  for key, value in feature_batch.items():
    print("   {!r:20s}: {}".format(key, value))

titanic_batches = tf.data.experimental.make_csv_dataset(
    titanic_file, batch_size=4,
    label_name="survived"
)

titanic_batches.element_spec

for feature_batch, label_batch in titanic_batches.take(1):
  print("Survived: {}".format(label_batch))
  print("features: ")
  for key, value in feature_batch.items():
    print(" {!r:20s}: {}".format(key, value))

titanic_batches = tf.data.experimental.make_csv_dataset(titanic_file, 4, label_name="survived", select_columns=['class', 'fare', 'survived'])

for feature_batch, label_batch in titanic_batches.take(1):
  print("survived: {}".format(label_batch))
  for key, value in feature_batch.items():
    print("   {!r:20s}: {}".format(key, value))

titanic_types = [tf.int32, tf.string, tf.float32, tf.int32, tf.int32, tf.float32, tf.string, tf.string, tf.string, tf.string]
dataset = tf.data.experimental.CsvDataset(titanic_file, titanic_types, header=True)
for line in dataset.take(10):
  print([item.numpy() for item in line])

# Commented out IPython magic to ensure Python compatibility.
# %%writefile missing.csv
# 1,2,3,4
# ,2,3,4
# 1,,3,4
# 1,2,,4
# 1,2,3,
# ,,,

record_defaults = [999, 999, 999, 999]
dataset = tf.data.experimental.CsvDataset("missing.csv",record_defaults=record_defaults)
dataset = dataset.map(lambda *items: tf.stack(items))
dataset

for line in dataset:
  print(line.numpy())

