{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TensorFlow-2.0-Custom-Training-tf.distribute.Strategy.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sourcecode369/TensorFlow-2.0/blob/master/tensorflow_2.0_docs/TensorFlow%20Core/Tutorials/Distributed%20Training/Custom%20Training%20Loop/TensorFlow_2_0_Custom_Training_tf_distribute_Strategy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GTJKg1j141p",
        "colab_type": "text"
      },
      "source": [
        "### Installing TensorFlow 2.0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbsG1pmKtTJI",
        "colab_type": "code",
        "outputId": "72ca02b6-709b-48f5-e0c6-44986f68fe45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        }
      },
      "source": [
        "!pip install --upgrade tensorflow-gpu"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: tensorflow-gpu in /usr/local/lib/python3.6/dist-packages (2.0.0)\n",
            "Requirement already satisfied, skipping upgrade: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.8.1)\n",
            "Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.8.0)\n",
            "Requirement already satisfied, skipping upgrade: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.0.8)\n",
            "Requirement already satisfied, skipping upgrade: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (3.1.0)\n",
            "Requirement already satisfied, skipping upgrade: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.11.2)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard<2.1.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (2.0.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.16.5)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (3.10.0)\n",
            "Requirement already satisfied, skipping upgrade: tensorflow-estimator<2.1.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (2.0.1)\n",
            "Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.1.7)\n",
            "Requirement already satisfied, skipping upgrade: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.33.6)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.2.2)\n",
            "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu) (2.8.0)\n",
            "Requirement already satisfied, skipping upgrade: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu) (41.2.0)\n",
            "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu) (3.1.1)\n",
            "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHg387GE18yT",
        "colab_type": "text"
      },
      "source": [
        "### Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRQz0pkBaxNa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, print_function, unicode_literals, division\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dtpcJ201_18",
        "colab_type": "text"
      },
      "source": [
        "### Setting up TensorFlow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NgcR3t_etxDr",
        "colab_type": "code",
        "outputId": "67aae679-5067-43b5-9200-6453f7c4816f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "print(\"TensorFlow version: \",tf.__version__)\n",
        "print(\"GPU is\",\"available\" if tf.test.is_gpu_available() else \"unavailable.\")\n",
        "print(\"TensorFlow is executing eagerly: \",tf.executing_eagerly())\n",
        "print(\"Random seeds set.\")\n",
        "tf.random.set_seed(1)\n",
        "print(\"Setting up TensorFlow soft device placement and device placement.\")\n",
        "tf.config.set_soft_device_placement(True)\n",
        "tf.debugging.set_log_device_placement(True)\n",
        "%load_ext tensorboard"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow version:  2.0.0\n",
            "GPU is available\n",
            "TensorFlow is executing eagerly:  True\n",
            "Random seeds set.\n",
            "Setting up TensorFlow soft device placement and device placement.\n",
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxbyh9Oh2DSr",
        "colab_type": "text"
      },
      "source": [
        "### Setting up the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eEZ5rfvPusN8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "\n",
        "train_images = train_images[..., None]\n",
        "test_images = test_images[..., None]\n",
        "\n",
        "train_images = train_images / np.float32(255)\n",
        "test_images = test_images / np.float32(255)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRB76hxOy6t0",
        "colab_type": "code",
        "outputId": "ab168fa0-1964-4478-b469-3cb62ea104ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(train_images.shape)\n",
        "print(test_images.shape)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28, 1)\n",
            "(10000, 28, 28, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HoqD4vr22Luz",
        "colab_type": "text"
      },
      "source": [
        "### Defining the tf.distribute.Strategy as Mirrorerd Strategy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1LaYIFSHy-7U",
        "colab_type": "code",
        "outputId": "e0cee6a4-8892-48ac-e9b5-84fe695d700d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "strategy = tf.distribute.MirroredStrategy(cross_device_ops=tf.distribute.HierarchicalCopyAllReduce())\n",
        "print(f\"Number of parallel devices: {strategy.num_replicas_in_sync}\")"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of parallel devices: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWwAhGomzTLc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BUFFER_SIZE = len(train_images)\n",
        "\n",
        "BATCH_SIZE_PER_REPLICA = 64\n",
        "GLOBAL_BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\n",
        "EPOCHS = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V33XCwwn2TIc",
        "colab_type": "text"
      },
      "source": [
        "### tf.data and strategy.experimental_distribute_dataset for setting up the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwYhqjtpzrr8",
        "colab_type": "code",
        "outputId": "913bfce8-5f47-4ea4-ce56-27fc6616e927",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels)).shuffle(BUFFER_SIZE).batch(GLOBAL_BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE) \n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels)).batch(GLOBAL_BATCH_SIZE) \n",
        "\n",
        "train_dist_dataset = strategy.experimental_distribute_dataset(train_dataset)\n",
        "test_dist_dataset = strategy.experimental_distribute_dataset(test_dataset)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Executing op TensorSliceDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
            "Executing op Equal in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op LogicalAnd in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op Select in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op AnonymousRandomSeedGenerator in device /job:localhost/replica:0/task:0/device:CPU:0\n",
            "Executing op ShuffleDatasetV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
            "Executing op BatchDatasetV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
            "Executing op PrefetchDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
            "Executing op RebatchDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
            "Executing op AutoShardDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
            "Executing op DeleteRandomSeedGenerator in device /job:localhost/replica:0/task:0/device:CPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3qENPXd0YBW",
        "colab_type": "code",
        "outputId": "97dcc130-29b8-4320-a9ed-85e6a76e3230",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(train_dist_dataset.element_spec)\n",
        "print(test_dist_dataset.element_spec)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.uint8, name=None))\n",
            "(TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.uint8, name=None))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udFSLs4A2bXJ",
        "colab_type": "text"
      },
      "source": [
        "### Defining the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXkdKiUM0fJD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model():\n",
        "  model = tf.keras.Sequential([\n",
        "      tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
        "      tf.keras.layers.MaxPooling2D(),\n",
        "      tf.keras.layers.Conv2D(64, 3, activation='relu'),\n",
        "      tf.keras.layers.MaxPooling2D(),\n",
        "      tf.keras.layers.Flatten(),\n",
        "      tf.keras.layers.Dense(64, activation='relu'),\n",
        "      tf.keras.layers.Dense(10, activation='softmax')\n",
        "    ])\n",
        "\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxdMPQQz1JzN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_dir = 'training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoWKdNGK1y3R",
        "colab_type": "text"
      },
      "source": [
        "### Defining the loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPLFYmYj13jp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with strategy.scope():\n",
        "  loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "      reduction=tf.keras.losses.Reduction.NONE)\n",
        "  def compute_loss(labels, predictions):\n",
        "    per_example_loss = loss_object(labels, predictions)\n",
        "    return tf.nn.compute_average_loss(per_example_loss, global_batch_size=GLOBAL_BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhHjRj5m-jKJ",
        "colab_type": "text"
      },
      "source": [
        "### Define the metrics to track loss and accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukFF46ZS-mgK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with strategy.scope():\n",
        "  test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
        "\n",
        "  train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
        "      name='train_accuracy')\n",
        "  test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
        "      name='test_accuracy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0iFYyJ8K_H-J",
        "colab_type": "text"
      },
      "source": [
        "### Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxLyayxg_K4w",
        "colab_type": "code",
        "outputId": "06d2f5a7-ffa5-4837-e0a1-0b6a7c298ec4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "with strategy.scope():\n",
        "  model = create_model()\n",
        "\n",
        "  optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "  # defining a checkpoint object\n",
        "  checkpoint = tf.train.Checkpoint(step=tf.Variable(1), optimizer=optimizer, model=model)\n",
        "  \n",
        "  # defining a checkpoint manager\n",
        "  manager = tf.train.CheckpointManager(checkpoint, './tf_ckpts', max_to_keep=3)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
            "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oP5otJcX_iZP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with strategy.scope():\n",
        "  \n",
        "  def train_step(inputs):\n",
        "    images, labels = inputs\n",
        "    with tf.GradientTape() as tape:\n",
        "      predictions = model(images, training=True)\n",
        "      loss = compute_loss(labels, predictions)\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    train_accuracy.update_state(labels, predictions)\n",
        "    return loss \n",
        "\n",
        "  def test_step(inputs):\n",
        "    images, labels = inputs\n",
        "    predictions = model(images, training=False)\n",
        "    t_loss = loss_object(labels, predictions)\n",
        "    test_loss.update_state(t_loss)\n",
        "    test_accuracy.update_state(labels, predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZvWoIR8x_nxA",
        "colab_type": "code",
        "outputId": "96ef56c6-2f28-46dd-c6f4-b255eecd98e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        }
      },
      "source": [
        "with strategy.scope():\n",
        "    @tf.function\n",
        "    def distributed_train_step(dataset_inputs):\n",
        "        per_replica_losses = strategy.experimental_run_v2(train_step,\n",
        "                                                          args=(dataset_inputs,))\n",
        "        return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses, axis=None)\n",
        "\n",
        "    @tf.function\n",
        "    def distributed_test_step(dataset_inputs):\n",
        "        return strategy.experimental_run_v2(test_step, args=(dataset_inputs,))    \n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        total_loss = 0.0\n",
        "        num_batches = 0\n",
        "\n",
        "        for x in train_dist_dataset:\n",
        "            total_loss += distributed_train_step(x)\n",
        "            num_batches += 1\n",
        "        train_loss = total_loss / num_batches\n",
        "\n",
        "        for x in test_dist_dataset:\n",
        "            distributed_test_step(x)\n",
        "\n",
        "        # distributed strategies dont support operations on checkpoint variables\n",
        "        checkpoint.step.assign_add(1)\n",
        "\n",
        "        # printing out the saved checkpoint epoch\n",
        "        if int(epoch)%2==0:\n",
        "            save_path = manager.save()\n",
        "            print(\"Saved checkpoint for step {}:{}\".format(int(epoch),save_path))\n",
        "        template = (\"Epoch {}, Loss {}, Accuracy {}, Test Loss {}, Test Accuracy {}\")\n",
        "        print(template.format(epoch+1, \n",
        "                          train_loss, \n",
        "                          train_accuracy.result()*100,\n",
        "                          test_loss.result(),\n",
        "                          test_accuracy.result() * 100\n",
        "                          ))\n",
        "        test_loss.reset_states()\n",
        "        train_accuracy.reset_states()\n",
        "        test_accuracy.reset_states"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Executing op GeneratorDataset in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op __inference_distributed_train_step_358389 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op __inference_distributed_train_step_368860 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op GeneratorDataset in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op __inference_distributed_test_step_369013 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op __inference_distributed_test_step_370488 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Saved checkpoint for step 0:./tf_ckpts/ckpt-2\n",
            "Epoch 1, Loss 0.2678895592689514, Accuracy 87.17208862304688, Test Loss 0.3373919725418091, Test Accuracy 88.44000244140625\n",
            "Executing op GeneratorDataset in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op GeneratorDataset in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Epoch 2, Loss 0.2432761937379837, Accuracy 91.00166320800781, Test Loss 0.2905786335468292, Test Accuracy 88.54833221435547\n",
            "Executing op GeneratorDataset in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op GeneratorDataset in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Saved checkpoint for step 2:./tf_ckpts/ckpt-3\n",
            "Epoch 3, Loss 0.2242988646030426, Accuracy 91.7316665649414, Test Loss 0.2689504027366638, Test Accuracy 88.77857208251953\n",
            "Executing op GeneratorDataset in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op GeneratorDataset in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Epoch 4, Loss 0.20732256770133972, Accuracy 92.30333709716797, Test Loss 0.26436522603034973, Test Accuracy 88.97250366210938\n",
            "Executing op GeneratorDataset in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op GeneratorDataset in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Saved checkpoint for step 4:./tf_ckpts/ckpt-4\n",
            "Epoch 5, Loss 0.1907876878976822, Accuracy 92.92832946777344, Test Loss 0.272710382938385, Test Accuracy 89.10333251953125\n",
            "Executing op GeneratorDataset in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op GeneratorDataset in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Epoch 6, Loss 0.174480140209198, Accuracy 93.55333709716797, Test Loss 0.28079986572265625, Test Accuracy 89.21700286865234\n",
            "Executing op GeneratorDataset in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op GeneratorDataset in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Saved checkpoint for step 6:./tf_ckpts/ckpt-5\n",
            "Epoch 7, Loss 0.16103190183639526, Accuracy 94.0433349609375, Test Loss 0.26972541213035583, Test Accuracy 89.35272979736328\n",
            "Executing op GeneratorDataset in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op GeneratorDataset in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Epoch 8, Loss 0.1468982845544815, Accuracy 94.61333465576172, Test Loss 0.2779768407344818, Test Accuracy 89.44249725341797\n",
            "Executing op GeneratorDataset in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op GeneratorDataset in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Saved checkpoint for step 8:./tf_ckpts/ckpt-6\n",
            "Epoch 9, Loss 0.13524669408798218, Accuracy 94.95499420166016, Test Loss 0.278940886259079, Test Accuracy 89.54615783691406\n",
            "Executing op GeneratorDataset in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op GeneratorDataset in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Epoch 10, Loss 0.124898761510849, Accuracy 95.33666229248047, Test Loss 0.26226285099983215, Test Accuracy 89.64928436279297\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QCg6ETOKMEF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "eval_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
        "    name = \"eval_accuracy\"\n",
        ")\n",
        "\n",
        "new_model = create_model()\n",
        "\n",
        "new_optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels)).batch(GLOBAL_BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQwwkFlbwSsu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def eval_step(images, labels):\n",
        "  predictions = new_model(images, training=False)\n",
        "  eval_accuracy(labels, predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2cQ2T6Yf7p3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b074696c-9476-441f-d053-3bd3eb75b082"
      },
      "source": [
        "print(f\"Final Checkpoints: {manager.checkpoints}\")"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Final Checkpoints: ['./tf_ckpts/ckpt-4', './tf_ckpts/ckpt-5', './tf_ckpts/ckpt-6']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3OyTLoZwdPl",
        "colab_type": "code",
        "outputId": "f0b6dfe4-a62d-4515-f9d5-15d329555a2f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# defining and restroring the latest checkpoint from the saved path through the predefined checkpoint manager\n",
        "checkpoint = tf.train.Checkpoint(step=tf.Variable(1),optimizer=new_optimizer, model=new_model)\n",
        "checkpoint.restore(tf.train.latest_checkpoint(manager.latest_checkpoint))\n",
        "\n",
        "for images, labels in test_dataset:\n",
        "  eval_step(images, labels)\n",
        "\n",
        "print ('Accuracy after restoring the saved model without strategy: {:.2f}'.format(\n",
        "    eval_accuracy.result()*100))"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy after restoring the saved model without strategy: 90.79\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTTjg9XxhIBG",
        "colab_type": "text"
      },
      "source": [
        "### Loading Mechanics\n",
        "\n",
        "> TensorFlow provides various loading mechanics.\n",
        "    \n",
        "    * Delayed Restorations\n",
        "    * Manually inspecting checkpoints\n",
        "    * List and dictionary tracking\n",
        "\n",
        "> TensorFlow matches variables to checkpointed values by traversing a directed graph with named edges, starting from the object being loaded. Edge names typically come from attribute names in objects, for example the \"l1\" in self.l1 = tf.keras.layers.Dense(5). tf.train.Checkpoint uses its keyword argument names, as in the \"step\" in tf.train.Checkpoint(step=...).\n",
        "\n",
        "##### Manually inspecting Checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZuS-PrAWw78f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        },
        "outputId": "a8aaa70f-1498-446e-f80d-035f54cc2fb8"
      },
      "source": [
        "tf.train.list_variables(tf.train.latest_checkpoint('./tf_ckpts/'))"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('_CHECKPOINTABLE_OBJECT_GRAPH', []),\n",
              " ('model/layer-0/bias/.ATTRIBUTES/VARIABLE_VALUE', [32]),\n",
              " ('model/layer-0/bias/.OPTIMIZER_SLOT/optimizer/m/.ATTRIBUTES/VARIABLE_VALUE',\n",
              "  [32]),\n",
              " ('model/layer-0/bias/.OPTIMIZER_SLOT/optimizer/v/.ATTRIBUTES/VARIABLE_VALUE',\n",
              "  [32]),\n",
              " ('model/layer-0/kernel/.ATTRIBUTES/VARIABLE_VALUE', [3, 3, 1, 32]),\n",
              " ('model/layer-0/kernel/.OPTIMIZER_SLOT/optimizer/m/.ATTRIBUTES/VARIABLE_VALUE',\n",
              "  [3, 3, 1, 32]),\n",
              " ('model/layer-0/kernel/.OPTIMIZER_SLOT/optimizer/v/.ATTRIBUTES/VARIABLE_VALUE',\n",
              "  [3, 3, 1, 32]),\n",
              " ('model/layer-2/bias/.ATTRIBUTES/VARIABLE_VALUE', [64]),\n",
              " ('model/layer-2/bias/.OPTIMIZER_SLOT/optimizer/m/.ATTRIBUTES/VARIABLE_VALUE',\n",
              "  [64]),\n",
              " ('model/layer-2/bias/.OPTIMIZER_SLOT/optimizer/v/.ATTRIBUTES/VARIABLE_VALUE',\n",
              "  [64]),\n",
              " ('model/layer-2/kernel/.ATTRIBUTES/VARIABLE_VALUE', [3, 3, 32, 64]),\n",
              " ('model/layer-2/kernel/.OPTIMIZER_SLOT/optimizer/m/.ATTRIBUTES/VARIABLE_VALUE',\n",
              "  [3, 3, 32, 64]),\n",
              " ('model/layer-2/kernel/.OPTIMIZER_SLOT/optimizer/v/.ATTRIBUTES/VARIABLE_VALUE',\n",
              "  [3, 3, 32, 64]),\n",
              " ('model/layer-5/bias/.ATTRIBUTES/VARIABLE_VALUE', [64]),\n",
              " ('model/layer-5/bias/.OPTIMIZER_SLOT/optimizer/m/.ATTRIBUTES/VARIABLE_VALUE',\n",
              "  [64]),\n",
              " ('model/layer-5/bias/.OPTIMIZER_SLOT/optimizer/v/.ATTRIBUTES/VARIABLE_VALUE',\n",
              "  [64]),\n",
              " ('model/layer-5/kernel/.ATTRIBUTES/VARIABLE_VALUE', [1600, 64]),\n",
              " ('model/layer-5/kernel/.OPTIMIZER_SLOT/optimizer/m/.ATTRIBUTES/VARIABLE_VALUE',\n",
              "  [1600, 64]),\n",
              " ('model/layer-5/kernel/.OPTIMIZER_SLOT/optimizer/v/.ATTRIBUTES/VARIABLE_VALUE',\n",
              "  [1600, 64]),\n",
              " ('model/layer-6/bias/.ATTRIBUTES/VARIABLE_VALUE', [10]),\n",
              " ('model/layer-6/bias/.OPTIMIZER_SLOT/optimizer/m/.ATTRIBUTES/VARIABLE_VALUE',\n",
              "  [10]),\n",
              " ('model/layer-6/bias/.OPTIMIZER_SLOT/optimizer/v/.ATTRIBUTES/VARIABLE_VALUE',\n",
              "  [10]),\n",
              " ('model/layer-6/kernel/.ATTRIBUTES/VARIABLE_VALUE', [64, 10]),\n",
              " ('model/layer-6/kernel/.OPTIMIZER_SLOT/optimizer/m/.ATTRIBUTES/VARIABLE_VALUE',\n",
              "  [64, 10]),\n",
              " ('model/layer-6/kernel/.OPTIMIZER_SLOT/optimizer/v/.ATTRIBUTES/VARIABLE_VALUE',\n",
              "  [64, 10]),\n",
              " ('optimizer/beta_1/.ATTRIBUTES/VARIABLE_VALUE', []),\n",
              " ('optimizer/beta_2/.ATTRIBUTES/VARIABLE_VALUE', []),\n",
              " ('optimizer/decay/.ATTRIBUTES/VARIABLE_VALUE', []),\n",
              " ('optimizer/iter/.ATTRIBUTES/VARIABLE_VALUE', []),\n",
              " ('optimizer/learning_rate/.ATTRIBUTES/VARIABLE_VALUE', []),\n",
              " ('save_counter/.ATTRIBUTES/VARIABLE_VALUE', []),\n",
              " ('step/.ATTRIBUTES/VARIABLE_VALUE', [])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8vAuDgIg5Fv",
        "colab_type": "text"
      },
      "source": [
        "### Alternate ways of iterating over a dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rtgdMa3igQ-",
        "colab_type": "text"
      },
      "source": [
        "#### Using Iterators"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iw0Pi1yalCB0",
        "colab_type": "text"
      },
      "source": [
        "> If we want to iterate over a given number of steps and not through the entire dataset we can create an iterator using the iter call and explicity call next on the iterator. <strong> We can choose to iterate over the dataset both inside and outside the tf.function</strong>. Here is a small snippet demonstrating iteration of the dataset outside the tf.function using an iterator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEjtB2kMk5aX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "67660a97-0867-4059-e8ed-d7b435aaba18"
      },
      "source": [
        "with strategy.scope():\n",
        "    for epoch in range(EPOCHS):\n",
        "        total_loss = 0.0\n",
        "        num_batches = 0\n",
        "        train_iter = iter(train_dist_dataset)\n",
        "\n",
        "        for _ in range(10):\n",
        "            total_loss += distributed_train_step(next(train_iter))\n",
        "            num_batches += 1\n",
        "        average_train_loss = total_loss / num_batches\n",
        "        template = (\"Epoch {}, Loss {}, Accuracy {}\")\n",
        "        print(template.format((epoch+1), average_train_loss, train_accuracy.result()*100))\n",
        "        train_accuracy.reset_states()"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Executing op GeneratorDataset in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Epoch 1, Loss 0.08790183067321777, Accuracy 96.328125\n",
            "Executing op GeneratorDataset in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Epoch 2, Loss 0.1114124059677124, Accuracy 95.78125\n",
            "Executing op GeneratorDataset in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Epoch 3, Loss 0.1382780224084854, Accuracy 94.53125\n",
            "Executing op GeneratorDataset in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Epoch 4, Loss 0.11071242392063141, Accuracy 95.78125\n",
            "Executing op GeneratorDataset in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Epoch 5, Loss 0.11459411680698395, Accuracy 95.46875\n",
            "Executing op GeneratorDataset in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Epoch 6, Loss 0.08854411542415619, Accuracy 97.1875\n",
            "Executing op GeneratorDataset in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Epoch 7, Loss 0.1039050966501236, Accuracy 96.25\n",
            "Executing op GeneratorDataset in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Epoch 8, Loss 0.08127101510763168, Accuracy 96.875\n",
            "Executing op GeneratorDataset in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Epoch 9, Loss 0.11283071339130402, Accuracy 96.09375\n",
            "Executing op GeneratorDataset in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Epoch 10, Loss 0.1101689338684082, Accuracy 95.15625\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14L1Lrz-mr82",
        "colab_type": "text"
      },
      "source": [
        "#### Iterating Inside a tf.function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TM20pJ4InGid",
        "colab_type": "text"
      },
      "source": [
        "> We can also iterate over the entire input train_dist_dataset inside a tf.function using the for x in ... construct or by creating iterators like we did above. The example below demonstrates wrapping one epoch of training in a tf.function and iterating over train_dist_dataset inside the function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vdXldvP8nTC9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "92c8b36d-0c71-4384-c36b-a5a02a2f9d1e"
      },
      "source": [
        "with strategy.scope():\n",
        "    @tf.function\n",
        "    def distributed_train_epoch(dataset):\n",
        "        total_loss = 0.0\n",
        "        num_batches = 0\n",
        "        for x in dataset:\n",
        "            per_replica_losses = strategy.experimental_run_v2(train_step, args=(x,))\n",
        "            total_loss += strategy.reduce(tf.distribute.ReduceOp.MEAN, per_replica_losses, axis=None)\n",
        "            num_batches += 1\n",
        "        return total_loss / tf.cast(num_batches,tf.float32)\n",
        "    \n",
        "    for epoch in range(EPOCHS):\n",
        "        train_loss = distributed_train_epoch(train_dist_dataset)\n",
        "        template = (\"Epoch {}, Loss {:.2f}, Accuracy {:.2f}\")\n",
        "        print(template.format((epoch+1), train_loss, train_accuracy.result()*100))\n",
        "        train_accuracy.reset_states()"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Executing op __inference_distributed_train_epoch_483235 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Epoch 1, Loss 0.11, Accuracy 95.75\n",
            "Epoch 2, Loss 0.10, Accuracy 96.17\n",
            "Epoch 3, Loss 0.10, Accuracy 96.40\n",
            "Epoch 4, Loss 0.09, Accuracy 96.78\n",
            "Epoch 5, Loss 0.08, Accuracy 96.99\n",
            "Epoch 6, Loss 0.07, Accuracy 97.11\n",
            "Epoch 7, Loss 0.07, Accuracy 97.63\n",
            "Epoch 8, Loss 0.06, Accuracy 97.73\n",
            "Epoch 9, Loss 0.06, Accuracy 97.80\n",
            "Epoch 10, Loss 0.05, Accuracy 98.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XS1I_pYQq9vw",
        "colab_type": "text"
      },
      "source": [
        "#### Tracking training loss across replicas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tUPC4O9sScS",
        "colab_type": "text"
      },
      "source": [
        "> We do not recommend using tf.metrics.Mean to track the training loss across different replicas, because of the loss scaling computation that is carried out.\n",
        "\n",
        "> For example, if you run a training job with the following characteristics: * Two replicas * Two samples are processed on each replica * Resulting loss values: [2, 3] and [4, 5] on each replica * Global batch size = 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCD4bsKhsulL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}