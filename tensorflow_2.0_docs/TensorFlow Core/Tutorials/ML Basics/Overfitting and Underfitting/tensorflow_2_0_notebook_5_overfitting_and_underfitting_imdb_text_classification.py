# -*- coding: utf-8 -*-
"""Tensorflow-2.0-Notebook_5 - Overfitting and Underfitting - IMDB Text Classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rtMSLsuyc_72xxEZsKiING74AuyZBZVZ
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# from __future__ import absolute_import, print_function, division, unicode_literals 
# %reload_ext autoreload
# %autoreload 2
# %reload_ext tensorboard
# 
# import scipy
# import numpy as np
# import pandas as pd
# import matplotlib.pyplot as plt
# import seaborn as sns
# sns.set(rc={"figure.figsize":(12,10)})
# sns.set_style("whitegrid")
# %matplotlib inline
# 
# import os
# import gc
# gc.enable()
# import sys
# import time
# import json
# import csv
# import random
# import datetime
# 
# try:
#   %tensorflow_version 2.x
#   print("TensorFlow 2.0 rc is up and running.")
# except:
#   print("There is some problem in initializing TensorFlow 2.0 rc")
# 
# import tensorflow as tf
# from tensorflow import keras
# from tensorflow.keras.callbacks import *
# from tensorflow.keras import layers
# from tensorflow.keras.utils import plot_model
# 
# print("TensorFlow version: ",tf.__version__)
# print("Eagerly Executing: ",tf.executing_eagerly())
# print("GPU is", "available." if tf.test.is_gpu_available() else "unavailable.")
# print("Seeding..")
# 
# def seedall(seed):
#   try:
#     np.random.seed(seed)
#     random.seed(seed)
#     tf.random.seed(seed)
#     os.environ["PYTHONHASHSEED"] = seed
#     print("Random Seeds have been initialized")
#   except:
#     print("Couldn't initialize the random seed.")
# 
# seedall(999)
# 
# print("Supressing warnings..")
# import warnings
# warnings.filterwarnings("ignore")
# print("Done")

NUM_WORDS = 10000
(train_data, train_labels), (test_data, test_labels) = keras.datasets.imdb.load_data(num_words=NUM_WORDS)

def multi_hot_sequences(sequences, dimension):
  results = np.zeros((len(sequences),dimension))
  for i, word_indices in enumerate(sequences):
    results[i, word_indices] = 1.0
  return results

train_data = multi_hot_sequences(train_data, dimension=NUM_WORDS)
test_data = multi_hot_sequences(test_data, dimension=NUM_WORDS)

plt.plot(train_data[0])

"""### Creating a baseline model"""

baseline_model = keras.Sequential([
    layers.Dense(16, activation=tf.nn.relu,kernel_initializer="he_uniform", input_shape=(NUM_WORDS,)),
    layers.Dense(16, activation=tf.nn.relu, kernel_initializer="he_uniform"),
    layers.Dense(1, activation=tf.nn.sigmoid )
])

baseline_model.compile(optimizer="adam",loss="binary_crossentropy",metrics=["accuracy",'binary_crossentropy'])

baseline_model.summary()

baseline_history = baseline_model.fit(train_data, 
                                      train_labels, 
                                      epochs=20, 
                                      batch_size=512, 
                                      validation_data=(test_data,test_labels))

result = baseline_model.evaluate(test_data, test_labels)
for name, value in zip(baseline_model.metrics_names,result):
  print('%s:%.3f'%(name, value))

plot_model(baseline_model)

"""### Samller Model"""

smaller_model = keras.Sequential([
    keras.layers.Dense(4, activation='relu', input_shape=(NUM_WORDS,)),
    keras.layers.Dense(4, activation='relu'),
    keras.layers.Dense(1, activation='sigmoid')
])

smaller_model.compile(optimizer='adam',
                      loss='binary_crossentropy',
                      metrics=['accuracy', 'binary_crossentropy'])

smaller_model.summary()

smaller_history = smaller_model.fit(train_data,
                                    train_labels,
                                    epochs=20,
                                    batch_size=512,
                                    validation_data=(test_data, test_labels),
                                    verbose=2)

"""### Bigger Model"""

bigger_model = keras.models.Sequential([
    keras.layers.Dense(512, activation='relu', input_shape=(NUM_WORDS,)),
    keras.layers.Dense(512, activation='relu'),
    keras.layers.Dense(1, activation='sigmoid')
])

bigger_model.compile(optimizer='adam',
                     loss='binary_crossentropy',
                     metrics=['accuracy','binary_crossentropy'])

bigger_model.summary()

bigger_history = bigger_model.fit(train_data, train_labels,
                                  epochs=20,
                                  batch_size=512,
                                  validation_data=(test_data, test_labels),
                                  verbose=2)

"""### Plot training and validation loss"""

def plot_history(histories, key="binary_crossentropy"):
  plt.figure(figsize=(16,10))
  for name, history in histories:
    val = plt.plot(history.epoch, history.history['val_'+key],
                   '--', label=name.title()+' Val')
    plt.plot(history.epoch, history.history[key], color=val[0].get_color(),
             label=name.title()+' Train')

  plt.xlabel('Epochs')
  plt.ylabel(key.replace('_',' ').title())
  plt.legend()

  plt.xlim([0,max(history.epoch)])

plot_history([
              ('baseline',baseline_history),
              ('smaller',smaller_history),
              ('bigger',bigger_history)
])

"""### Strategies to Prevent Overfitting"""

# 1. Weight Regularization

l2_model = keras.Sequential([
                             layers.Dense(16, kernel_initializer="glorot_uniform",kernel_regularizer=keras.regularizers.l2(0.001),activation="relu",input_shape=(NUM_WORDS,)),
                             layers.Dense(16, kernel_initializer="glorot_uniform",kernel_regularizer=keras.regularizers.l2(0.001),activation="relu"),
                             layers.Dense(1,activation="sigmoid")
])

l2_model.compile(optimizer="adam", loss="binary_crossentropy",metrics=["accuracy","binary_crossentropy"])

l2_model_history = l2_model.fit(train_data, 
                                train_labels,
                                epochs=20,
                                batch_size=512,
                                validation_data=(test_data, test_labels),
                                verbose=2)

#2 Dropout

dropout_model = keras.Sequential([
                             layers.Dense(16, kernel_initializer="glorot_uniform",activation="relu",input_shape=(NUM_WORDS,)),
                             layers.Dropout(0.5),
                             layers.Dense(16, kernel_initializer="glorot_uniform",activation="relu"),
                             layers.Dropout(0.5),
                             layers.Dense(1,activation="sigmoid")
])

dropout_model.compile(optimizer="adam", loss="binary_crossentropy",metrics=["accuracy","binary_crossentropy"])

dropout_model_history = dropout_model.fit(train_data, 
                                train_labels,
                                epochs=20,
                                batch_size=512,
                                validation_data=(test_data, test_labels),
                                verbose=2)

#3. BatchNormalization
normalize_model = keras.Sequential([
                             layers.Dense(16, kernel_initializer="glorot_uniform",activation="relu",input_shape=(NUM_WORDS,)),
                             layers.BatchNormalization(),
                             layers.Dense(16, kernel_initializer="glorot_uniform",activation="relu"),
                             layers.BatchNormalization(),
                             layers.Dense(1,activation="sigmoid")
])

normalize_model.compile(optimizer="adam", loss="binary_crossentropy",metrics=["accuracy","binary_crossentropy"])

normalize_model_history = normalize_model.fit(train_data, 
                                train_labels,
                                epochs=20,
                                batch_size=512,
                                validation_data=(test_data, test_labels),
                                verbose=2)

### All regularizers

regularized_model = keras.Sequential([
                             layers.Dense(16, kernel_regularizer=keras.regularizers.l2(0.001) , kernel_initializer="glorot_uniform",activation="relu",input_shape=(NUM_WORDS,)),
                             layers.Dropout(0.5),
                             layers.BatchNormalization(),
                             layers.Dense(16, kernel_regularizer=keras.regularizers.l2(0.001), kernel_initializer="glorot_uniform",activation="relu"),
                             layers.Dropout(0.5),
                             layers.BatchNormalization(),
                             layers.Dense(1,activation="sigmoid")
])

regularized_model.compile(optimizer="adam", loss="binary_crossentropy",metrics=["accuracy","binary_crossentropy"])

regularized_model_history = regularized_model.fit(train_data, 
                                train_labels,
                                epochs=20,
                                batch_size=512,
                                validation_data=(test_data, test_labels),
                                verbose=2)

plot_history([
              ('baseline',baseline_history),
              ('smaller',smaller_history),
              ('bigger',bigger_history),
              ("l2 regularlized", l2_model_history),
              ("dropout regularized", dropout_model_history),
              ("batch normalization regularized", normalize_model_history),
              ("all regularized",regularized_model_history)
])

