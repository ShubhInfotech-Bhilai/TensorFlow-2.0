# -*- coding: utf-8 -*-
"""TensorFlow_2_0_Notebook_2_Cats_and_Dogs_Image_Classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11sxOI-H872rHLoxbhOJEtkphGsbwT4AV
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# from __future__ import absolute_import, division, unicode_literals, print_function
# %reload_ext autoreload
# %reload_ext tensorboard
# %autoreload 2
# 
# import cv2
# import numpy as np
# import pandas as pd
# import seaborn as sns
# from PIL import Image
# from scipy import stats
# import matplotlib.pyplot as plt
# from matplotlib import gridspec as gridspec
# sns.set(rc={"figure.figsize":(12,10)})
# sns.set_style("whitegrid")
# %matplotlib inline
# %config InlineBackend.figure_format = "retina"
# #np.set_print_options(precision=3,subprocess=True)
# 
# import os
# import gc
# gc.enable()
# import sys
# import csv
# import json
# import time
# import glob
# import random
# import pathlib
# import datetime
# import functools
# import subprocess
# import multiprocessing
# from multiprocessing import Pool
# from tqdm import tqdm_notebook, tqdm
# 
# import warnings
# warnings.filterwarnings("ignore")
# warnings.simplefilter("ignore")
# 
# import tensorflow as tf
# from tensorflow import keras
# from tensorflow.keras import layers
# from tensorflow.keras import metrics
# from tensorflow.keras import losses
# from tensorflow.keras import optimizers
# from tensorflow.keras.callbacks import *
# 
# import sklearn
# from sklearn.preprocessing import *
# from sklearn.model_selection import *
# from sklearn.metrics import *
# from sklearn.ensemble import *
# 
# try:
#   %tensorflow_version 2.x
#   print("TensorFlow is up and running.")
# except:
#   print("Unable to run TensorFlow.")
# 
# print("Tensorflow version: ",tf.__version__)
# print("Tenosrflow is executing eagerly: ", tf.executing_eagerly())
# print("GPU is","available." if tf.test.is_gpu_available() else "unavaialable.")
# 
# print("Seeding...")
# def seed_all(seed):
#   try:
#     np.random.seed(seed)
#     random.seed(seed)
#     tf.random.set_seed(seed)
#     print("Random seeds initialized.")
#   except:
#     print("Uable to initialize seeds.")
# seed_all(999)
# 
# from IPython.display import display, clear_output
# from IPython.core.interactiveshell import InteractiveShell
# InteractiveShell.ast_node_interactivity = "all"
# print(os.listdir())
# print("Done.!")

_URL = 'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip'

path_to_zip = tf.keras.utils.get_file('cats_and_dogs.zip', origin=_URL, extract=True)

PATH = os.path.join(os.path.dirname(path_to_zip), 'cats_and_dogs_filtered')

print(os.listdir())

train_dir = os.path.join(PATH, 'train')
validation_dir = os.path.join(PATH, 'validation')

train_cats_dir = os.path.join(train_dir, 'cats')  
train_dogs_dir = os.path.join(train_dir, 'dogs')  
validation_cats_dir = os.path.join(validation_dir, 'cats')  
validation_dogs_dir = os.path.join(validation_dir, 'dogs')

num_cats_tr = len(os.listdir(train_cats_dir))
num_dogs_tr = len(os.listdir(train_dogs_dir))

num_cats_val = len(os.listdir(validation_cats_dir))
num_dogs_val = len(os.listdir(validation_dogs_dir))

total_train = num_cats_tr + num_dogs_tr
total_val = num_cats_val + num_dogs_val

print('total training cat images:', num_cats_tr)
print('total training dog images:', num_dogs_tr)

print('total validation cat images:', num_cats_val)
print('total validation dog images:', num_dogs_val)
print("--")
print("Total training images:", total_train)
print("Total validation images:", total_val)

batch_size = 64
epochs = 20
IMG_HEIGHT = 150
IMG_WIDTH = 150

train_image_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255,
                                                                        rotation_range=15,
                                                                        width_shift_range=0.1,
                                                                        height_shift_range=0.1,
                                                                        shear_range=0.01,
                                                                        zoom_range=[0.9, 1.25],
                                                                        horizontal_flip=True,
                                                                        vertical_flip=True,
                                                                        fill_mode='reflect',
                                                                        data_format='channels_last',
                                                                        brightness_range=[0.5, 1.5],
                                                                        validation_split=0.3)
validation_image_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)

train_data_gen = train_image_generator.flow_from_directory(train_dir, 
                                                           target_size=(IMG_HEIGHT, IMG_WIDTH),
                                                           color_mode="rgb",
                                                           batch_size=batch_size,
                                                           shuffle=True, 
                                                           class_mode='binary')

validation_data_gen = validation_image_generator.flow_from_directory(validation_dir, 
                                                           target_size=(IMG_HEIGHT, IMG_WIDTH),
                                                           color_mode="rgb",
                                                           batch_size=batch_size,
                                                           shuffle=True, 
                                                           class_mode='binary')

sample_training_images, _ = next(train_data_gen)

# plt.figure(figsize=(30,30))
# for i in range(sample_training_images.shape[0]):
#   plt.subplot(16,8,i+1)
#   plt.imshow(sample_training_images[i,:,:,:],interpolation="nearest")
#   plt.xticks([])
#   plt.yticks([])
#   plt.grid(True)
# plt.tight_layout()
# plt.show()

from tensorflow.keras.layers import *
from tensorflow.keras.applications import ResNet50, InceptionV3
from tensorflow.keras import *

def create_model():
  base_model = InceptionV3(weights='imagenet', include_top=False)
  x = base_model.output
  x = GlobalAveragePooling2D()(x)
  x = Dense(512, activation='relu')(x) 
  predictions = Dense(1, activation='sigmoid')(x) 
  model = Model(base_model.input, predictions)
  for layer in base_model.layers:
        layer.trainable = False
  model.compile(optimizer="adam",loss="binary_crossentropy",metrics=["acc"])
  return model

base_clf = create_model()
base_clf.summary()
keras.utils.plot_model(base_clf)

class PlotLearning(tf.keras.callbacks.Callback):
  def on_train_begin(self, logs={}):
    self.i = 0
    self.x = []
    self.loss = []
    self.val_loss = []
    self.acc = []
    self.val_acc = []
    self.fig = plt.figure()
    self.logs = []

  def on_epoch_end(self, epoch, logs={}): 
    self.logs.append(logs)
    self.x.append(self.i)
    self.loss.append(logs.get('loss'))
    self.acc.append(logs.get('acc'))
    self.val_loss.append(logs.get('val_loss'))        
    self.val_acc.append(logs.get('val_acc'))
    self.i += 1
    f, ax = plt.subplots(1, 2, figsize=(18,10), sharex=True)
    ax = ax.flatten()
    clear_output(wait=True)
    ax[0].plot(self.x, self.loss, label="Binary Cross Entropy", lw=2)
    ax[0].plot(self.x, self.val_loss, label="Val Binary Cross Entropy")
    ax[0].legend()
    ax[0].grid(True)
    ax[1].plot(self.x, self.acc, label="Accuracy", lw=2)
    ax[1].plot(self.x, self.val_acc, label="Val Accuracy")
    ax[1].legend()
    ax[1].grid(True)
    plt.show();

reduce_lr = ReduceLROnPlateau(monitor="val_loss",factor=0.3,patience=3,verbose=1,mode="auto",min_lr=0.000001)
checkpoint = ModelCheckpoint("base_clf.h5",monitor="val_loss",verbose=1,save_best_only=True,mode="auto")
early_stopping = EarlyStopping(monitor="val_loss",patience=6,mode="auto")
plotLoss = PlotLearning()

history =  base_clf.fit_generator(train_data_gen, 
                                  steps_per_epoch=total_train//batch_size,
                                  epochs=epochs,
                                  validation_data=validation_data_gen,
                                  validation_steps=total_val // batch_size,
                                  callbacks=[reduce_lr, checkpoint, early_stopping, plotLoss])

"""### Using RAdam Optimizer"""

from keras.optimizers import Optimizer
from keras import backend as K


class RAdam(Optimizer):

    def __init__(self, lr, beta1=0.9, beta2=0.99, decay=0, **kwargs):
        super(RAdam, self).__init__(**kwargs)
        with K.name_scope(self.__class__.__name__):
            self.lr = K.variable(lr)
            self._beta1 = K.variable(beta1, dtype="float32")
            self._beta2 = K.variable(beta2, dtype="float32")
            self._max_sma_length = 2 / (1 - self._beta2)
            self._iterations = K.variable(0)
            self._decay = K.variable(decay)

    def get_updates(self, loss, params):
        grads = self.get_gradients(loss, params)
        self.updates = [K.update_add(self._iterations, 1)]
        first_moments = [K.zeros(K.int_shape(p), dtype=K.dtype(p))
                         for (i, p) in enumerate(params)]
        second_moments = [K.zeros(K.int_shape(p), dtype=K.dtype(p))
                          for (i, p) in enumerate(params)]

        self.weights = [self._iterations] + first_moments + second_moments
        bias_corrected_beta1 = K.pow(self._beta1, self._iterations)
        bias_corrected_beta2 = K.pow(self._beta2, self._iterations)
        for i, (curr_params, curr_grads) in enumerate(zip(params, grads)):
            # Updating moving moments

            new_first_moment = self._beta1 * first_moments[i] + (
                    1 - self._beta1) * curr_grads
            new_second_moment = self._beta2 * second_moments[i] + (
                    1 - self._beta2) * K.square(curr_grads)
            self.updates.append(K.update(first_moments[i],
                                         new_first_moment))
            self.updates.append(K.update(second_moments[i],
                                         new_second_moment))

            # Computing length of approximated SMA

            bias_corrected_moving_average = new_first_moment / (
                    1 - bias_corrected_beta1)
            sma_length = self._max_sma_length - 2 * (
                    self._iterations * bias_corrected_beta2) / (
                                 1 - bias_corrected_beta2)

            # Bias correction

            variance_rectification_term = K.sqrt(
                self._max_sma_length * (sma_length - 4) * (sma_length - 2) / (
                        sma_length * (self._max_sma_length - 4) *
                        (self._max_sma_length - 2) + K.epsilon()))
            resulting_parameters = K.switch(
                sma_length > 5, variance_rectification_term *
                bias_corrected_moving_average / K.sqrt(
                    K.epsilon() + new_second_moment / (1 -
                                                       bias_corrected_beta2)),
                bias_corrected_moving_average)
            resulting_parameters = curr_params - self.lr * resulting_parameters
            self.updates.append(K.update(curr_params, resulting_parameters))
        if self._decay != 0:
            new_lr = self.lr * (1. / (1. + self._decay * K.cast(
                self._iterations, K.dtype(self._decay))))
            self.updates.append(K.update(self.lr, new_lr))
        return self.updates

    def get_config(self):
        config = {
            "lr": float(K.get_value(self.lr)),
            "beta1": float(K.get_value(self._beta1)),
            "beta2": float(K.get_value(self._beta2)),
        }
        base_config = super(RAdam, self).get_config()
        return dict(list(base_config.items()) + list(config.items()))

def create_radam_model():
  base_model = InceptionV3(weights='imagenet', include_top=False)
  x = base_model.output
  x = GlobalAveragePooling2D()(x)
  x = Dense(1024, activation='relu')(x) 
  predictions = Dense(1, activation='sigmoid')(x) 
  model = Model(base_model.input, predictions)
  for layer in base_model.layers:
        layer.trainable = False
  model.compile(optimizer=RAdam(1e-4),loss="binary_crossentropy",metrics=["acc"])
  return model

radam_clf = create_model()
radam_clf.summary()
keras.utils.plot_model(radam_clf)

reduce_lr = ReduceLROnPlateau(monitor="loss",factor=0.3,patience=3,verbose=1,mode="auto",min_lr=0.000001)
checkpoint = ModelCheckpoint("radam_clf.h5",monitor="loss",verbose=1,save_best_only=True,save_weights_only=True,mode="auto")
early_stopping = EarlyStopping(monitor="loss",patience=10,mode="auto")
plotLoss = PlotLearning()

history =  radam_clf.fit_generator(train_data_gen, 
                                  steps_per_epoch=total_train//batch_size,
                                  epochs=25,
                                  validation_data=validation_data_gen,
                                  validation_steps=total_val // batch_size,
                                  callbacks=[reduce_lr, checkpoint, early_stopping, plotLoss])

